<!doctype html><html><head><meta name=generator content="Hugo 0.119.0"><title>Canadian Computational Neuroscience Spotlight</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=no"><link rel=stylesheet href=https://krembilneuroinformatics.github.io/ccns/assets/css/main.css><noscript><link rel=stylesheet href=https://krembilneuroinformatics.github.io/ccns/assets/css/noscript.css></noscript><style>:root{--site-background:url("https://krembilneuroinformatics.github.io/ccns/images/new_canada_2.jpg")}</style><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script></head><body class=is-preload><div id=wrapper><header id=header><div class=logo><a href=/><span class="icon fa-"></span></a></div><div class=content><div class=inner><h1>Canadian Computational Neuroscience Spotlight</h1><p>V4 taking place October 5-6 2023. Register at crowdcast.io/c/ccnsv4</p></div></div><nav><ul><li><a href=#01-about-ccns>01-About CCNS</a></li><li><a href=#02-organizing-committee>02 - Organizing Committee</a></li><li><a href=#03-speaker-spotlight>03- Speaker Spotlight</a></li><li><a href=#04-ccns-2023-program>04 - CCNS 2023 Program</a></li><li><a href=#05-past-future>05 - Past & Future</a></li></ul></nav></header><div id=main><article id=01-about-ccns><h2 class=major>01-About CCNS</h2><span class="image main"><img src alt></span><p>The Canadian Computational Neuroscience Spotlight (CCNS) was created following the mass cancellations and postponements
of traditional neuroscience conferences during the early stages of the COVID-19 pandemic, including two such meetings
amongst the Canadian neuroscience community. The absence of these meetings presented an opportunity to create a brand-new,
entirely virtual academic meeting that could take full advantage of the online setting. Given that traditionally-defined
trainees and early-career researchers were arguably most impacted by the cancellation of the networking and learning
opportunities that conferences present, CCNS was designed as a “trainee-focused” meeting, highlighted by tutorial talks
beginning each session, panel discussions with both established and early-career scientists, and a spotlight on trainee
presentations.</p><p>The <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008485">first edition of CCNS</a> was planned and implemented entirely in ten weeks and yielded a meeting with more than 450
registrants, including representation from every continent across the globe. Perhaps most importantly, the limited costs
of the virtual setting allowed the meeting to be completely free of charge for all attendees. Every element of the meeting
remains available for replay online, another benefit of the virtual setting. This success served as the impetus for making
CCNS a recurring academic meeting.</p><p>Going forward, CCNS will continue to highlight cutting-edge computational neuroscience research, both in Canada and around
the world, while providing unique learning, networking, and presentation opportunities for early-career researchers. The
meeting is committed to remaining cost-accessible to the entire academic community, using the virtual setting to maximize
accessibility for populations for which physical conferences present a challenge, and maintaining a gender-balanced and
diverse lineup of speakers during its continued evolution.</p></article><article id=02-organizing-committee><h2 class=major>02 - Organizing Committee</h2><span class="image main"><img src alt></span><figure class="image main"><img src=images/screenshot.png></figure><h2 id=about-the-organizing-committee>About the Organizing Committee</h2><p><a href=http://scottrich.strikingly.com/><strong>Dr. Scott Rich</strong></a> (<a href=https://twitter.com/RichCompNeuro>@RichCompNeuro</a>, <em><strong>CCNS lead organizer</strong></em>) is currently a Research Fellow at SickKids Research Institute and an <a href="https://twitter.com/RichCompNeuro/status/1676958486146715652?s=20">incoming Assistant Professor at the University of Connecticut&rsquo;s Department of Physiology and Neurobiology in January 2024</a>.
The Rich Lab will use a wide range of tools from computational neuroscience to study how neuronal heterogeneity and diversity drives physiologically relevant brain function, and how disruptions to this diversity might contribute to various neuropathologies.</p><p><a href=cognemo.com><strong>Dr. Andreea Diaconescu</strong></a> (<a href=https://twitter.com/cognemo_andreea>@cognemo_andreea</a>, <em><strong>CCNS co-organizer</strong></em>) is currently an Independent Scientist at the Krembil Centre for Neuroinformatics at the Centre for Addiction and Mental Health. Leading the Cognitive Network Modelling team, her research focuses on developing cognitive tasks and computational models that address specific symptoms in psychiatry with a particular focus on delusions. In combination with neuroimaging and electrophysiological recordings, the aim is to assess the clinical utility of these models in prospective patient studies</p><p><a href=grifflab.com><strong>Dr. John Griffiths</strong></a> (<a href=https://twitter.com/neurodidact>@neurodidact</a>, <em><strong>CCNS co-organizer</strong></em>) is a cognitive and computational neuroscientist, with particular research interests in mathematical modelling of large-scale neural dynamics, multimodal neuroimaging data analysis methods, and brain stimulation in the context of neuropsychiatric and neurological disease. He is currently an Independent Scientist at the Krembil Centre for Neuroinformatics at CAMH, where he leads a team focused on whole-brain and multi-scale neurophysiological modelling. He is also an Assistant Professor in the University of Toronto Departments of Psychiatry and Institute of Medical Sciences.</p><p><a href=https://sites.google.com/view/lnsbsp/home><strong>Dr. Milad Lankarany</strong></a> (<a href=https://twitter.com/MLankarany>@MLankarany</a>, <em><strong>CCNS co-organizer</strong></em>) is an Assistant Professor at the University of Toronto Instiute of Biomaterials and Biomedical Engineering, and a Scientist at the Krembil Brain Institute. The main focus of his lab&rsquo;s work is to uncover information processing mechanisms of neural systems. His goal is to understand how information is represented, propagated and computed. Understanding neural information processing will result in the development of computational algorithms and engineering techniques for the optimal controlling of the functionality of neural systems. For example, closed loop neuro-stimulators can be used to adaptively intervene with the neural system of patients with neurological disorders in order to restore the normal activity.</p></article><article id=03-speaker-spotlight><h2 class=major>03- Speaker Spotlight</h2><span class="image main"><img src alt></span><figure class="image main"><img src=images/CCNSv4Speakers.png></figure><h2 id=ccnsv4-keynote-speakers>CCNSv4 Keynote Speakers</h2><p><a href="https://iris.ucl.ac.uk/iris/browse/profile?upi=RAADA06"><strong>Dr. Rick Adams</strong></a>: Dr. Rick Adams studied medicine and neuroscience at Cambridge University and did his PhD under Prof Karl Friston in UCL. He did post doctoral fellowships with Prof Jon Roiser and Prof Janaina Mourao-Miranda (both UCL) and now is a Future Leaders Fellow with his own lab spanning the Centre for Medical Image Computing and Institute of Cognitive Neuroscience at UCL. He also works as a Consultant Psychiatrist specialising in psychosis and neuropsychiatry. His research interest is in using computational modelling of both behaviour and brain imaging data to try to understand underlying cognitive or neurobiological processes contributing to psychosis spectrum disorders.</p><p><a href=https://www.uottawa.ca/faculty-science/professors/jeremie-lefebvre><strong>Dr. Jeremie Lefebvre</strong></a>: Jeremie Lefebvre is an Associate Professor at the Department of Biology and Physics of the University of Ottawa and Affiliate Scientist at the Krembil Research Institute. He also holds appointments at the Department of Mathematics of the University of Toronto. Prof Lefebvre was trained in Ottawa, Nancy, Geneva and Lausanne, developing his expertise in non-linear dynamics and neurophysiology. His team uses interdisciplinary methods to better understand the roles of noise, non-linearity and diversity in brain function – and what goes wrong in diseases such as epilepsy and multiple-sclerosis.</p><p><a href=https://sites.google.com/site/sarahfeldtmuldoon/><strong>Dr. Sarah Muldoon</strong></a>: Dr. Sarah Muldoon is an Associate Professor in the Mathematics Department, Interim Director of the Graduate Program in Computational and Data-Enabled Science within the Institute for Artificial Intelligence and Data Science, and member of the Neuroscience Program at the University at Buffalo, SUNY. Her research interests lie at the intersection of experiment and theory with a focus on applications of network theory to neuroscience data. She has spent extensive time working in experimental neurobiology labs and now runs a research group that couples theoretical advancement, computational modeling, and data-intensive analysis to study the relationship between structure and function in brain networks.</p></article><article id=04-ccns-2023-program><h2 class=major>04 - CCNS 2023 Program</h2><span class="image main"><img src alt></span><h3 id=ccnsv4-will-take-place-on-october-5-6-2023-and-registration-is-open-herehttpswwwcrowdcastiocccnsv4>CCNSv4 will take place on October 5-6, 2023, and registration is open <a href=https://www.crowdcast.io/c/ccnsv4>here</a>.</h3><h1 id=program>Program</h1><h3 id=thursday-october-5>Thursday, October 5</h3><ul><li>9:00 AM EDT: <strong>Welcome and Introduction: Dr. Scott Rich</strong></li><li>9:15 AM EDT: <strong>Keynote: Dr. Rick Adams</strong>- Using biophysical modelling to determine excitatory, inhibitory and/or NMDA receptor dysfunction in psychosis</li><li>10:15 AM EDT: <strong>Break</strong></li><li>10:30 AM EDT: <strong>Tutorial: Dr. Andreea Diaconescu</strong></li><li>11:00 AM EDT: <strong>Trainee Spotlight Talks</strong></li><li>11:30 AM EDT: <strong>Parallel Trainee Talks</strong></li><li>12:30 PM EDT: <strong>Lunch Break</strong></li><li>1:30 PM EDT: <strong>Panel Discussion</strong></li><li>2:15 PM EDT: <strong>Tutorial: Dr. Axel Hutt</strong></li><li>2:45 PM EDT: <strong>Break</strong></li><li>3:00 PM EDT: <strong>Keynote: Dr. Jeremie Lefebvre</strong></li><li>4:00 PM EDT: <strong>Trainee Spotlight Talks</strong></li><li>4:30 PM EDT: <strong>Parallel Trainee Talks</strong></li></ul><h3 id=friday-october-6>Friday, October 6</h3><ul><li>9:00 AM EDT: <strong>Trainee Spotlight Talks</strong></li><li>9:30 AM EDT: <strong>Parallel Trainee Talks</strong></li><li>10:00 AM EDT: <strong>Break</strong></li><li>10:15 AM EDT: <strong>Keynote: Dr. Sarah Muldoon</strong>- Personalized Brain Network Models</li><li>11:15 AM EDT: <strong>Tutorial: Dr. Scott Rich</strong></li><li>11:45 AM EDT: <strong>Concluding Remarks</strong></li></ul><h2 id=trainee-abstracts>Trainee Abstracts</h2><h3 id=spotlight-thursday-1100-1130-am-edt>Spotlight, Thursday 11:00-11:30 AM EDT</h3><ul><li><strong>Alexandre Guet-McCreight</strong>: Reduced inhibition by somatostatin-expressing interneurons is associated with depression. Administration of positive allosteric modulators of α5 subunit-containing GABAA receptor (α5-PAM) that selectively target this lost inhibition exhibit antidepressant and pro-cognitive effects in rodent models of chronic stress. However, the functional effects of α5-PAM on the human brain in vivo are unknown, and currently cannot be assessed experimentally. We modeled the effects of α5-PAM on tonic inhibition as measured in human neurons, and tested in silico α5-PAM effects on detailed models of human cortical microcircuits in health and depression. We found that α5-PAM effectively recovered impaired cortical processing as quantified by stimulus detection metrics, and also recovered the power spectral density profile of the microcircuit EEG signals. We performed an α5-PAM dose response and identified simulated EEG biomarker candidates. Our results serve to de-risk and facilitate α5-PAM translation and provide biomarkers in non-invasive brain signals for monitoring target engagement and drug efficacy.</li><li><strong>Andrea Luppi</strong>: Patterns of neural activity underlie human cognition. Transitions between these patterns are orchestrated by the brain’s network architecture. What are the mechanisms linking network structure to cognitively relevant activation patterns? Here we implement principles of network control to investigate how the architecture of the human connectome shapes transitions between 123 experimentally defined cognitive activation maps (cognitive topographies) from the NeuroSynth meta-analytic engine. We also systematically incorporate neurotransmitter receptor density maps (18 receptors and transporters) and disease-related cortical abnormality maps (11 neurodegenerative, psychiatric and neurodevelopmental diseases; N = 17 000 patients, N = 22 000 controls). Integrating large-scale multimodal neuroimaging data from functional MRI, diffusion tractography, cortical morphometry, and positron emission tomography, we simulate how anatomically-guided transitions between cognitive states can be reshaped by pharmacological or pathological perturbation. Our results provide a comprehensive look-up table charting how brain network organisation and chemoarchitecture interact to manifest different cognitive topographies. This computational framework establishes a principled foundation for systematically identifying novel ways to promote selective transitions between desired cognitive topographies.</li></ul><h3 id=spotlight-thursday-400-430-pm-edt>Spotlight, Thursday 4:00-4:30 PM EDT</h3><ul><li><strong>Laura Green</strong>: Intrinsic activity of a circuit shapes the way we encode information and respond to our environment. Recent work suggests disinhibition plays a role in shaping place cell activity in the hippocampal CA1 region. From a computational perspective, disinhibition provides a mechanism for self-sustained activity, suggesting attractor-like activity in CA1 without the need for recurrent excitation. Most previous studies on attractor dynamics utilize recurrent excitation, as is the case in the CA3 region. In turn, it is tacitly assumed that the target CA1 region functions simply as a relay or output node, without exploiting the computational properties of CA1 circuitry. In a reduced firing rate model, we confirm that disinhibition coupled with broad lateral inhibition can lead to attractor dynamics. That is, with spatially uniform and strong enough input to excitatory and inhibitory cells the distributed rate model is capable of a spatially-localized ‘bump’ of activity. We then develop a spiking population model of CA1 place cell activity based on the same circuitry. We model place cell activity along a linear track in response to spatiotemporal input. Compared to all-to-all circuitry, the model with disinhibition offers improved signal-to-noise and more stable maintenance of temporal organization of place cell activity in response to changes in input. We further extend the model to demonstrate that disinhibition can sustain intrinsic sequential activity. These observations are consistent with known functions of attractor models and proposed roles of the hippocampus, to compress information while maintaining encodability.</li><li><strong>Shervin Safavi</strong>: Multiple studies have reported signatures of criticality observed in various neuronal recordings. Moreover, theoretical investigations demonstrate that multiple aspects of information processing are optimized at the second-order phase transition. These studies motivated the hypothesis that the brain operates close to a critical state. While it has been shown that several computational aspects of sensory information processing are optimal in this regime, it is still unclear whether these computational benefits of criticality can be leveraged by neural systems for performing behaviorally relevant computations [3]. To address this question, we consider a network of leaky integrate-and-fire neurons that is optimized for efficient coding. Previously, it was shown that the performance of such networks varies non-monotonically with the noise amplitude [4]. We consider networks with different noise amplitudes and evaluate how close they are to the critical state by measuring deviations from the nearest power law of avalanche size distributions. We find that in the vicinity of the optimal noise level for efficient coding, the network dynamics exhibits signatures of criticality, namely, the distribution of avalanche sizes is closest to a power law. When the noise amplitude is too low or too high for efficient coding, the network appears either super-critical or sub-critical, respectively. Lastly, we verify the stability of this result to changes in the network’s size and observed a similar behavior. This result has important implications, as it shows how two influential, and previously disparate fields - efficient coding, and criticality - might be intimately related.</li></ul><h3 id=spotlight-friday-900-930-am-edt>Spotlight, Friday 9:00-9:30 AM EDT</h3><ul><li><strong>Yuhua Yu</strong>: Temporal variability is a fundamental property of brain processes and functionally important to human cognition. This study examined how fluctuations in neural oscillatory activity are related to problem-solving performance. We used volatility to assess the step-by-step fluctuations of EEG spectral power while individuals attempted to solve word-association puzzles. Inspired by recent results with hidden-state modeling, we tested the hypothesis that spectral-power volatility is directly associated with problem-solving outcomes. As predicted, volatility was lower during trials solved with insight compared to those solved analytically. Moreover, volatility during pre-stimulus preparation for solving predicted general problem-solving outcomes, including solving-success and solving-time. These novel findings were replicated in a separate dataset from an anagram-solving task, suggesting that less-rapid transitions between neural oscillatory synchronization and desynchronization predict better solving performance and are conducive to solving with insight. Thus, volatility can be a valuable index to characterize brain dynamics relevant to cognition.</li><li><strong>Carol Upchurch</strong>: Persistent activity in excitatory principal cells has been suggested as a mechanism to maintain memory traces during working memory. Persistent interruption of firing in PV+ inhibitory interneurons has been demonstrated and suggested to be the inhibitory counterpart of persistent activity in excitatory neurons. Since an interruption in inhibitory interneuronal firing disinhibits the excitatory principal cells, it could serve a similar function. We find the mechanism is similar in models of neocortical and hippocampal area CA1 fast spiking interneurons. An inhibitory synaptic potential or hyperpolarizing current that decays sufficiently slowly is applied to a repetitively firing interneuron. This stimulus removes inactivation from rapidly activating, slowly inactivating potassium current KV1, bringing the membrane potential to a temporary equilibrium above the spike threshold during repetitive firing. After the stimulus has faded, KV1 continues to de-inactivate slowly until KV1 is sufficiently weak that repetitive firing can be reestablished. A reduced model using the inactivation variable as a bifurcation parameter shows the initial stimulus collapses the limit cycle for repetitive firing onto a co-existing stable fixed point corresponding to depolarization block. As this variable decays, the trajectory crosses a supercritical Hopf bifurcation following the curve of unstable fixed points until it spirals out into repetitive firing. In an alternative model describing entorhinal cortical PV+ interneurons, similar behavior is achieved without a slowly activating/inactivating gated current. When the baseline current is close enough to a Hopf bifurcation, the fixed point is weakly repelling, taking hundreds of milliseconds to resume firing after an interruption.</li></ul><h3 id=parallel-a-thursday-1130-am-1230-pm-edt>Parallel A, Thursday 11:30 AM-12:30 PM EDT</h3><ul><li><strong>Aref Pariz</strong>: Selective control of synaptic plasticity and stabilizing post-stimulation oscillatory dynamics in neuronal populations. Transcranial Electrical Stimulation (tES) and transcranial magnetic stimulation (tMS) are non-invasive treatments for neurological and neuropsychiatric disorders such as chronic pain or major depression and have received increased attention in the past decades. Although amplified interest and reports about their effectiveness, little is known about the way they engage and interfere with both individual and populations of neurons. Ubiquitous neural diversity and heterogeneity, related to morphology, function, and intrinsic cellular features, result in widely distinctive responses to stimuli, and thus may well influence the effectiveness of therapeutic approaches. Through the modulation of endogenous oscillations, transcranial alternating current stimulation (tACS) technique may engage synaptic plasticity, hopefully leading to persistent lasting effects. we examined how spike-timing-dependent plasticity, at the level of cells, intra- and inter-laminar cortical networks, can be selectively and preferentially engaged by periodic stimulation. Using leaky integrate-and-fire neuron models, in silico, we analyzed cortical circuits comprised of multiple cell-types, alongside superficial multi-layered networks expressing distinct layer-specific timescales. Our results show that mismatch in neuronal timescales within and/or between cells—and the resulting variability in excitability, temporal integration properties, and frequency tuning—enables selective and directional control of synaptic connectivity by tACS. We also have explored the conditions on which tACS leads to amplified post-stimulation oscillatory power, persisting once stimulation has been turned off.</li><li><strong>Thomas Wahl</strong>: Mental disorders may exhibit pathological brain rhythms and neurostimulation promises to alleviate of patients&rsquo; symptoms by modifying these rhythms. Today, most neurostimulation schemes are open-loop, i.e. administer experimental stimulation protocols independent of the patients brain activity which may yield a sub-optimal treatment. We propose a closed-loop feedback control scheme estimating an optimal stimulation based on observed brain activity. The optimal stimulation is chosen according to a user-defined target frequency distribution, which permits frequency tuning of the brain activity in real-time. The mathematical description details the major control elements and applications to biologically realistic simulated brain activity illustrate the scheme&rsquo;s possible power in medical practice. The proposed neurostimulation control theme promises to permit the medical personnel to tune a patient&rsquo;s brain activity in real-time.</li><li><strong>Maliha Ahmed</strong>: Childhood absence epilepsy (CAE) is an idiopathic generalized epilepsy disorder which affects children be- tween the ages of 4-12 years. It is characterized by sudden brief periods of impaired consciousness occurring several times a day. One of the most confounding features of CAE is its ability to spontaneously resolve in adoles- cence in roughly 80% of cases, while in others it can progress into more severe types of epilepsy. There remains an inadequate understanding of some of the factors involved in remission that can inform early intervention practices. According to some functional connectivity studies, there exist pre-treatment connectivity differences between pa- tients who ultimately experience remission and those who do not, namely increased frontal cortical connections at the time of diagnosis. There is also substantial evidence on the effect of sex steroids on human (and animal) brain rhythmic discharges, particularly the effect of progesterone metabolite allopregnanolone on the action of gamma- Aminobutyric acid (GABA), an inhibitory neurotransmitter. We have developed a computational model of the thalamocortical network to study the role of allopregnanolone on network behaviour in the eventual resolution of CAE. The results from this research can possibly better inform therapeutic decisions relating to early interventions tailored to intrinsic network connectivity arrangements, which can be evaluated prior to the start of treatment and at the time of diagnosis.</li><li><strong>Anath Vedururu Srinivas</strong>: Phase resetting Curves (PRCs) have been useful in determining and analyzing various phase-locking modes in oscillatory neurons under pulse-coupling assumption. Previously, our group used PRCs to determine the stability of synchrony both within and between clusters of neural oscillators, considering within and between cluster terms separately, without synaptic delays. Subsequently, the interactions of the within and between cluster terms were considered, demonstrating how an alternating firing pattern between clusters could stabilize within cluster synchrony even in clusters unable to synchronize themselves in isolation. Later, criteria were derived for synchrony between two pulse-coupled oscillators with synaptic delays. In this study, we derived stability criteria for synchrony in one and two clusters of pulse-coupled oscillators by including delays in transmission of inputs between oscillators. We demonstrated the validity of these results using a map of firing intervals based on the PRC. We used self-connected neurons to represent clusters and therefore derived conditions under which a neuron can phase-lock itself with a delayed input. For one cluster, we used a realistic model of a fast-spiking PV+ interneurons in the medial entorhinal cortex. For the other, we used an RTM model of an excitatory neuron. Although this analysis only strictly applies to identical neurons receiving identical synapses from the same number of neurons, the principles are general and can be used to understand how to promote or impede synchrony in physiological networks of neurons. The results from this work can help understand how excitatory neurons promote synchrony in the network of fast-spiking inhibitory neurons.</li><li><strong>Arthur Powanwe</strong>: Activity in primary visual cortex induced by a static stimulus exhibits a sharp power transient over a broad frequency range that settles into a highly variable gamma rhythm (30-90 Hz). What dynamical regimes underlie such responses of the neural circuitry, and whether brain state transitions are caused by such stimuli, are ongoing open questions. Taking advantage of a recent envelope-phase description of pyramidal-inhibitory network gamma (PING) rhythms, along with maximum likelihood estimation, we fit local field potential data to two meta-parameters that enable pinpointing the dynamical regime established by the external stimuli. The neuronal noise in this efficient model accounts for the random ”gamma burst” epochs of higher gamma power and trial to trial variability. This framework establishes how low-to-moderate stimuli can toggle the cortex from a noisy non-oscillatory fixed point to a noise-driven stable focus attractor (i.e. a quasi-cycle) with high Fisher information; gamma bursts arise in an Inhibition-Stabilized network (ISN) regime with near-zero net synaptic currents. The responses to increasing stimuli elicit a mix of normal and non-normal amplifications. But a strong stimulus moves the system beyond the ISN, resulting in the weak gamma response seen in the data. Our analysis explains the transient at stimulus onset as well as the correlation between stimulus strength and mean burst duration, network frequency, firing rates and effective synaptic coefficients. We conclude that a sensory input can induce a change of dynamical regime in the cortex.</li></ul><h3 id=parallel-b-thursday-1130-am-1230-pm-edt>Parallel B, Thursday 11:30 AM-12:30 PM EDT</h3><ul><li><strong>Dmitrii Todorov</strong>: Motor adaptation is a critical aspect of human motor control and has implications for stroke rehabilitation. We investigated its neural correlates using magnetoencephalography in healthy participants performing reaching movements with a joystick. Participants adapted visuomotor perturbations presented either in a stable manner across trials or changing randomly. During stable blocks, participants successfully adapted to the perturbation. Classical behavioral models of motor adaptation assume that adaptation depends only on the most recent trial, which is not always enough. Alternative approaches assume dependence on several recent trials. Usually, it is done by making the learning rate dynamic and there are multiple approaches to do so. In H. Tan model learning rate is based on error variability and in D. Herzfeld model the learning rate is based on error sign consistency. The relatively high number of model parameters involved in these models and other confounding cognitive processes present during adaptation make it difficult to directly compare these models. Neural representations of such models behaviors are not extensively studied. Specifically it is not clear how the learning rate is stored in the brain. Our study addresses both issues: we directly relate brain recordings to internal variables of computational models and this, in turn, allows us to compare the models in terms of their connection to neural dynamics. Our models predict reaching error on the next trial and we used decoding accuracy of this error as a benchmark.</li><li><strong>Yohan Yee</strong>: The Allen Human Brain Atlas (AHBA) consists of genome-wide gene expression sampled at 3702 locations across the brains of six donors. In computational neuroscience, it has opened the door to including transcriptomic information in biophysical models, which allows, for example, the modeling of whole brain dynamics that depend on spatially varying cytoarchitecture. The wide use of the AHBA in neuroscience is a result of tissue sample locations being reported in Montreal Neurological Institute (MNI) space, a common coordinate system in neuroimaging. These reported coordinates follow from the alignment of donor brains to an MNI template brain by the Allen Institute; a separate set of “updated” MNI coordinates have also been derived within the alleninf software packages by others. Here, we show that there are substantial inaccuracies in these two previously reported sets of MNI coordinates, and many of the 3702 samples are misclassified in their location. We derive a new set of coordinates using multispectral image alignment of donor brains to the modern MNI152 symmetric 2009c template and show that our new coordinates provide dramatically better image intensity correlations between source (donor) and target (template) brains as compared to previous reported coordinates, implying a better registration and mapping of sample coordinates to MNI space. Using an anatomical atlas, we further assign samples to structures based on their reported coordinates and (using expert annotations as ground truth) demonstrate that fewer samples are mislabeled when using our new coordinates. These results suggest that using previously-reported coordinates may result in inaccurate inferences.</li><li><strong>Leanne Monteiro</strong>: The human visual cortex is >20% larger in males than in females, yet, no studies have addressed sex differences during its development. Here, we looked at the molecular development of the human visual cortex to see if the mechanisms that govern growth and experience-dependent plasticity differed between sexes. Using proteomic and genomic databases with post-mortem tissue samples (age range: 20 days to 80 years), we characterized the development of regulatory mechanisms in the visual cortex, including 23 synaptic proteins, 72 neuroimmune proteins (n=30, F=12) and 58 myelin-associated genes (n=48, F=21). By applying a data-driven approach, we found that 7 trajectories capture the range of development. Unexpectedly, we found that females and males exhibit distinct trajectory patterns of molecular changes across the lifespan, suggesting that they use different mechanisms to regulate experience-dependent developmental plasticity. Differential Expression Sliding Window Analysis (DE-SWAN) was applied to unpack the proteins and genes driving the age- and sex-related differences. We found many sex differences in childhood when waves of development separated the protein and gene expression in females and males. Smaller waves of differences occurred in adolescence and older adults. The significant sex differences in childhood suggest that the female and male visual cortex may use different mechanisms to regulate experience-dependent developmental plasticity. Furthermore, these findings underscore the pressing need for a more diverse approach to visual neuroscience research.</li><li><strong>Laura Medlock</strong>: Our understanding of somatosensory processing in rodents comes primarily from the whisker system whereas the coding properties of low-threshold mechanoreceptors (LTMRs) in rodent glabrous skin have yet to be well characterized. An LTMR’s response to touch is influenced by stimulus intensity (rate coding) and frequency (temporal coding). Rate and temporal coding are influenced by the probability of a spike occurring on each cycle (i.e. spike reliability) and the timing of spikes relative to the stimulus cycle (i.e. spike precision), respectively. Accordingly, through in vivo extracellular recordings in rodents, we measured the reliability and precision of LTMR responses to sinusoidal vibrotactile stimuli between 2 and 300 Hz. LTMRs were first classified as rapid adapting (RA) or slow adapting (SA) based on their response to sustained pressure. Heterogeneity in the response of RAs to vibration revealed a spectrum of frequency preferences across this LTMR subtype. Furthermore, although stimulus frequency differentially affected spike reliability across different RAs, increasing frequency universally increased spike precision. Finally, to explore the mechanisms supporting the unique tuning properties of rodent LTMRs, we fit generalized linear models to experimental data. Our models reproduced experimental reliability and precision and demonstrated that the integration time window of different RAs transitions from wide to narrow as tuning preference across the population moved from low to high frequencies. Together, these experimental results strengthen our understanding of somatosensory processing in rodents, and the resulting models allow us to efficiently dissect the coding properties of different LTMR subtypes.</li><li><strong>Skerdi Progri</strong>: The claustrum is a thin sheet of subcortical grey matter bordered laterally by the insula and putamen. Being implicated with numerous basic and higher-order tasks and various neurological and psychiatric disorders, claustral investigation in humans is of high interest among neuroscientists. Yet its sub-millimetre thinness and intricate morphology make the claustrum difficult to visualize in vivo. As a 3D reference of the human claustrum was lacking, we decided to establish the first gold standard model by manually parcellating the claustrum from histological data at 100µm isotropic resolution, using ITK-SNAP. Utilizing the high resolution afforded by histological data, we managed to capture the claustrum’s full extent. As such, our model was larger than predicted from previous low-resolution studies. Additionally, we found our gold standard to match claustral descriptions from literature, including the sub-millimetre mediolateral thinness and discontiguous ventral regions. This model was then used as a guide for semi-automated claustral parcellation in data from 6 subjects acquired on an ultra-high-field MRI system at 0.7mm isotropic resolution. To the same subject data, we applied available automated segmentation algorithms claiming to capture the claustrum and compared results with our semi-automated segmentation. Automated segmentations either overestimated or underestimated claustral size when compared to our gold-standard-guided segmentation. The accuracy of our semi-automated segmentation emphasized the necessity of a gold standard and revealed the promise of our model to serve as a guide for future functional studies seeking to establish empirical evidence for hypothesized claustral function and involvement in neurological and psychiatric disorders.</li></ul><h3 id=parallel-c-thursday-1130-am-1230-pm-edt>Parallel C, Thursday 11:30 AM-12:30 PM EDT</h3><ul><li><strong>Lioba Berndt</strong>: Psychosis, closely linked to conditions like schizophrenia and bipolar disorder, poses significant challenges in detection and treatment, especially in high-risk individuals. This study explores the use of event-related potential components, specifically mismatch negativity (MMN) derived from electroencephalography (EEG), as a tool for modeling differential brain effective connectivity in clinical high-risk of psychosis (CHR-P) individuals. CHR-P often exhibit reduced MMN amplitudes. The study&rsquo;s novelty lies in using EEG data to establish new biomarkers related to psychosis risk and deepen our understanding of its underlying causes. Dynamic Causal Modeling (DCM) was applied to EEG data from the North American Prodrome Longitudinal Study (NAPLS2), with two main goals: uncovering differences in effective connectivity between Healthy Controls and CHR-P individuals and distinguishing between CHR-P Converters (CHR-P(Conv)) and CHR-P Non-Converters (CHR-P(Non-Conv)). The modeling identified disparities in brain connectivity among these groups, primarily stemming from regional connectivity shifts within specific brain regions rather than disconnections between sources. Key parameters contributing to MMN differences, like reduced self-inhibition in the left superior temporal gyrus, increased self-inhibition in spiny stellate cells within the left STG, and altered inhibitory connections within the left primary auditory cortex, were extracted and aligned with MMN variations. These findings suggest that the model successfully replicates observed MMN differences, hinting at possible causal mechanisms. The revealed neuronal parameter differences hold promise as predictive markers for psychosis development. Furthermore, the research highlights the effectiveness of DCM in uncovering neural distinctions between CHR-P(Conv) and CHR-P(Non-Conv), emphasizing its utility in exploring the neurobiological mechanisms underlying psychosis conversion.</li><li><strong>Matthew D. Bachman</strong>: Decision making frequently requires calculating and combining the different attributes of each option. Prior research has characterized the spatial distribution of multi-attribute decision making by applying machine learning algorithms to fMRI data. Notably, these studies have reported activation in perceptual regions in addition to those more commonly involved in the calculation of value. However, fMRI’s low temporal resolution limits our ability to disassociate between the relative timing of perceptual and value signals. Our study fills this gap by applying machine learning algorithms to high-temporal resolution EEG data collected during a multi-attribute decision making task. To accomplish this, 40 participants first learned to associate different faces and colors with varying amounts of positive and negative values. Participants then saw combined face-color stimuli, which they had to accept or reject based on the summed value of the face and color attributes. These points were later converted into money to incentivize participants to accurately accept positive total values and reject negative ones. Linear support vector machine classifiers were trained by the EEG data during the choice period to decode when it could significantly predict activation for each attribute type. Our results indicate that the perceptual attributes for faces and colors could be significantly decoded earlier than for their associated values. However, value attributes were maintained for longer duration. Activation for the integrated face-color value was the slowest condition and was maintained after the stimulus disappeared. These findings suggest a temporal distinction between the formation of perceptual and value signals in multi-attribute decision making.</li><li><strong>Yi (Jason) Yang</strong>: Background: Amotivation in schizophrenia (SZ) is difficult to assess and associated with poor clinical/functional outcomes. Our research utilizes Bayesian computational models to objectively analyze exploratory behavior, providing insights into the underlying mechanisms of amotivation in schizophrenia. Methods: We analyzed data of 24 SZ and 26 healthy control (HC) who performed the Virtual Novelty Exploratory Task (VNET). Statistical analysis was performed on 25 behavioral features for group comparison and correlation with clinical measures to give us intuition about our model. Using the Hierarchical Gaussian Filter, we modeled participants’ exploratory behavior under uncertainty and evaluated model parameters to assess group differences in exploratory behavior in relation to amotivation. Results: No correlation was observed between AES and task performance. However, SANS-Amot was correlated with “Approach Distance” (p=0.05, r=-0.55), and “Exploratory Walking Distance” (p=0.004, r=-0.56). Also, the SZ group spent less time “Scanning Environment” (p = 0.025). Further, computational analysis found higher kappa values in the SZ group (p=0.005), and relationships between SANS-Amot with β_2 (p=0.006, r=0.54) and μ_2 (p=0.009, r=-0.52). Discussion: SZ patients, indicated by higher kappa values, are more impacted by environmental changes and have more uncertainty about familiar/novel objects. Despite this, they do not exhibit a reduced β_2 value, showing no inherent desire to reduce uncertainty. This, combined with more amotivated SZ patients having elevated β_2 scores, suggests SZ group lacked an innate drive to reduce uncertainty through exploration. Our research highlights the computational approach&rsquo;s value in probing amotivation and suggests exploratory behaviors can measure amotivation severity in SZ patients.</li><li><strong>Hyuna Cho</strong>: Many real-world decisions require recalling information that is not immediately available in the environment. The sampling of memory during choice has been theorized (Shadlen & Shohamy, 2016), but remains understudied. Here, we investigate whether and how we prioritize which memories to sample. Participants (N=36) completed a 3 stage learning and memory task. First, participants learned to associate positive/negative values with faces and scenes. Second, participants learned to associate face-scene pairs (either value-congruent or incongruent) with object images. Finally, participants were presented with objects alone and had to use it to decide whether to accept a 50/50 chance of obtaining either the associated face or scene value. Critically, some trials multiplied either face or scene values by x2 or x3, increasing the importance of that attribute. We hypothesized that more important attributes should be recalled earlier in the memory sampling process. Supporting this hypothesis, a drift diffusion model with an early memory accumulation process better fit choices than one without (DIC=191). Model parameters suggested that early face- (but not scene-) related sampling led to more attribute-biased choices (𝛽earlyFace=3.30, p&lt;.001) and better decisions (𝛽earlyFace=3.10, p&lt;.001). Separately, we found that face values were better remembered (t(28)paired=0.286, p=.003), and asymmetrically utilized during value-incongruent trials (74.3%), suggesting a unique role for memorability in early memory sampling. However, this early sampling was unaffected by externally-cued attribute importance. Our preliminary work yields novel insights into memory retrieval dynamics during choice.</li><li><strong>Mohamad Abdelhack</strong>: Sleep, depression, and cognition exhibit complex relationships. Depression is associated with both insomnia and hypersomnia. Sleep deprivation was also shown to be an effective anti-depressant for a subset of populations despite being known to impair cognition. In this study, we seek to untangle these conundrums. We analyzed neural activations from visual task-based and resting-state fMRI data of over 30,000 participants from the UK-Biobank cohort. We then modeled their associations with accelerometer-measured duration of longest sleep bout, self-reported depression score, cognition task score, self-reported insomnia, and daytime dozing. We investigated the correlation between the association patterns for each of the five phenotypes. We then additionally analyzed over 800 subjects’ data from the human connectome project (HCP) dataset. Task-based analysis revealed neural signatures of increased depression symptoms and frequency of insomnia and daytime dozing to correlate negatively with those of the length of sleep duration and cognitive ability. Resting-state data, however, showed a reversed relationship for the duration of longest sleep bout and both depression and insomnia where the neural signatures correlated positively which was corroborated by the HCP dataset. The contradictory pattern was driven mainly by the long-sleeper participants. Brain-wide analysis of task and resting connectivity further revealed that insomnia and depression were associated with hypoconnectivity in task-based data but hyperconnectivity in resting-state. Our results reveal that resting neural signature of insomnia and depression resemble those of long sleep challenging the conventional wisdom and suggesting a more complex relationship between different sleep and depression phenotypes.</li><li><strong>Haoxin Zhang</strong>: Enhanced memory for emotional experiences is hypothesized to depend on amygdala-hippocampal interactions during memory consolidation. Here we show using intracranial recordings from the human amygdala and the hippocampus during an emotional memory encoding and discrimination task increased awake ripples after encoding of emotional, compared to neutrally-valenced stimuli. Further, post-encoding ripple-locked stimulus similarity was predictive of later memory discrimination. Ripple-locked stimulus similarity appeared earlier in the amygdala than in hippocampus and mutual information analysis confirmed amygdala influence in hippocampal activity. Finally, the joint ripple-locked stimulus similarity in the amygdala and hippocampus was predictive of correct memory discrimination. These findings provide electrophysiological evidence that post-encoding ripples enhance memory for emotional events.</li></ul><h3 id=parallel-a-thursday-430-510-pm-edt>Parallel A, Thursday 4:30-5:10 PM EDT</h3><ul><li><strong>Ankit Roy</strong>: Neurons regulate their average firing rate close to a set-point despite perturbations . They do so by adjusting their a) synaptic strength or b) somatic excitability. Homeostatic regulation of these two properties has been the focus of significant research but certain issues remain unresolved; for instance, both properties utilize changes in intracellular calcium as a feedback signal to direct compensatory changes, but a single feedback signal cannot simultaneously regulate two properties. Notably, past computational studies have implicitly focused on global calcium changes because neurons were modeled as single, homogeneous compartments, yet separate studies have shown that calcium can change differently in different parts of a neuron, transiently reaching high concentrations within spatially restricted domains, unlike the subtler changes that occur globally. This insight leads to our hypothesis that local calcium signals can independently encode different error signals and thereby support independent regulation of >1 properties. We tested this hypothesis in an abstract two compartmental model with dendrite and soma containing calcium-dependent homeostatic mechanisms. We showed that separate calcium signals allow the dendritic and somatic compartment to separately implement compensatory mechanisms. But the same could not be achieved by a single global calcium signal. However, in real neurons due to ion diffusion, calcium signals are never completely separate. Thus, we tested our hypothesis in a realistic multicompartmental neuron model with experimentally determined biophysical constraints. Using this model, we showed that local calcium signals are separate enough to support independent homeostatic regulations in the neuron.</li><li><strong>Nooshin Abdollahi</strong>: Myelinated axons propagate spikes efficiently by regenerating spikes at each node of Ranvier, a process known as saltatory conduction. Previous studies have shown that saltatory conduction is affected by the intrinsic properties of the axon, including morphological, passive electrical, and active electrical properties. Besides properties of the axon itself, saltatory conduction may also be affected by extracellular conditions, including its passive electrical properties. Indeed, neurons are routinely modeled with the assumption that extracellular space is connected to ground, resulting in infinite extracellular conductivity; in many cases (including in intact nerve), that assumption is invalid. By extension, transmembrane voltage is often calculated under the assumption that extracellular voltage is zero, but this too is inaccurate. How extracellular current flow under realistically resistive conditions affects spike propagation has been overlooked in previous studies. Using computational models of axons, we found that an axon with the same intrinsic properties might have different conduction velocity depending on factors such as its location within a loosely packed or densely packed fascicle, or the spatial arrangement of the nodes of Ranvier of the nearby fibers. We explain this result by tracking the flow of current intracellularly and extracellularly. Moreover, this effect interacts with intrinsic axon properties, meaning certain intrinsic factors like myelin thickness can have a large or small effect on velocity depending on the extracellular resistivity. Overall, our results demonstrate that the resistivity of the extracellular space impacts spike propagation and must be considered, especially when trying to gauge the importance of other, intrinsic factors.</li><li><strong>Joanna Slawinska</strong>: A framework for data assimilation and prediction of nonlinear dynamics is presented, combining aspects of quantum mechanics, Koopman operator theory, and kernel methods for machine learning. This approach adapts the formalism of quantum dynamics and measurement to perform data as- similation (filtering), using the Koopman operator governing the evolution of observables as an analog of the Heisenberg operator in quantum me- chanics, and a quantum mechanical density operator to represent the data assimilation state. The framework is implemented in a fully empirical, data- driven manner by representing the evolution and measurement operators via matrices in a basis learned from time-ordered observations. Applications to data assimilation of the Lorenz 96 multiscale system and others show promising results. Extensions of this work toward spatiotemporal pattern extraction and subgrid-scale modeling is presented as well. Furthermore, our framework provides a route for implementing and designing appropriate algorithms on quantum computers.</li><li><strong>Marina Chugunova</strong>: Gonadotropin-releasing hormone (GnRH) neuron exhibits two modes of exocytosis: pulsatile mode and a surge. In rodents, the surge of GnRH occurs once in five days and triggers a chain of physiological processes that leads to an ovulation in female species. We hypothesize that the switch from the pulsatile mode to a surge is caused by the synaptic facilitation of the calcium channels. This synaptic facilitation is induced by the action potential that propagates from the initiation site to the distal dendron (dendrite with the partial axon properties). We consider several types of synaptic facilitation. As an additional trigger for the synaptic facilitation of the calcium channels, we consider a direct affect of the estradiol on the distal part of the GnRH neuron’s dendron. The computational model of the process reproduces the bursting of the membrane potential that correlates with the experimental data. The corresponding bursting-type oscillation of the calcium ions concentration with the changing oscillation frequency is the cause of the surge of the GnRH into a portal circulatory system.</li></ul><h3 id=parallel-b-thursday-430-510-pm-edt>Parallel B, Thursday 4:30-5:10 PM EDT</h3><ul><li><strong>Meike van der Heijden</strong>: The cerebellum contributes to a diverse array of motor conditions including ataxia, dystonia, and tremor. The neural substrates that encode this diversity are unclear. Here, we tested whether the neural spike activity of cerebellar output neurons predicts the phenotypic presentation of cerebellar pathophysiology. Using in vivo recordings as input data, we trained a supervised classifier model to differentiate the spike parameters between mouse models for ataxia, dystonia, and tremor. The classifier model correctly predicted mouse phenotypes based on single neuron signatures. Spike signatures were shared across etiologically distinct but phenotypically similar disease models. Mimicking these pathophysiological spike signatures with optogenetics induced the predicted motor impairments in otherwise healthy mice. These data show that distinct spike signatures promote the behavioral presentation of cerebellar diseases.</li><li><strong>Kant (Heng Kang) Yao</strong>: Reduced inhibition from somatostatin (SST) interneurons and dendritic spine loss in pyramidal (Pyr) neurons are implicated in underlying cognitive deficits in treatment-resistant depression. Cortical SST interneurons primarily inhibit the apical dendrites of Pyr neurons to facilitate lateral inhibition and modulate integration of synaptic signals received by dendritic spines. To study the effects of reduced inhibition from SST interneurons and spine loss on dendritic stimulus processing, we expanded our previous data-driven models of human cortical microcircuits in health and depression to include active dendritic properties that enable backpropagating action potentials as measured in human neurons, and we included spine loss in depression in terms of synapses loss and altered intrinsic property. We then characterized the joint functional implications of reduced SST interneuron inhibition and spine loss in depression on dendritic processing of stimuli in terms of the signal-to-noise ratio, stimulus detection errors, and the non-linear dendrites integration of synaptic inputs. Our study thus mechanistically links inhibition changes in depression to deficits in dendritic processing in human cortical microcircuits.</li><li><strong>Roman Pagodin</strong>: Synaptic weight distributions are typically log-normal [Buzsáki and Mizuseki, 2014], which has been linked to multiplicative plasticity mechanisms [Loewenstein et al., 2011]. In contrast, deep learning models rely on gradient descent, which is not multiplicative. A growing literature in computational neuroscience relies on deep learning models to study plasticity [Richards et al, 2019], but it is not clear if the gradient descent-based learning rules are consistent with log-normal weight distributions. In this work, we show that synaptic weight distributions depend on the choice of distance for synaptic changes, i.e. the geometry of synaptic plasticity. Using the theoretical tools provided by mirror descent, we establish the dependency between synaptic weight distributions and geometry. For gradient descent, this geometry is Euclidean, and typically produces Gaussian rather than log-normal weights. However, other gradient-based algorithms with non-Euclidean geometry, such as exponentiated gradient, can reliably produce log-normal distributions. These conclusions depend on the initial weight distributions, but we show that it should be possible to experimentally test for different synaptic geometries by looking at the distribution of weight changes after learning a task. Importantly, our analysis also works with error-driven Hebbian learning, such as 3-factor Hebbian rules [Frémaux and Gerstner, 2016]. In simulations, we show that our theory applies to both standard feedforward convolutional networks, and also more biological recurrent models of the visual stream.</li><li><strong>Pankaj Gupta</strong>: Recurrent-Neural-Network (RNN) models aim to dissociate inputs from self v/s other brain regions in a task-specific manner (Perich et al., 2021). We adapted this approach to model the large-scale Visual Behavior Brain-Observatory GCaMP6f functional imaging datasets from the Allen Institute (Hu et al., 2021; Orlova et al., 2020; de Vries et al., 2020; Garrett et al., 2020; Groblewski et al., 2020) which contains curated 2-photon recordings from multiple visual areas (Primary Visual Area ie. VISp, Anterolateral visual area ie. VISal, Anteromedial visual area ie. VISam, and Lateral visual area ie. VISl), from multiple cortical layers, different cell types (Pyramidal ie. PC, Somatostatin ie. SST, vasoactive intestinal polypeptide ie. VIP), over six days post-training, in a total of 19 mice performing a visual image change detection task. A multi-region RNN was iteratively trained and constrained to fit the biological neural data at each step, mimicking the activity of in vivo neurons. After the RNN is trained, we read out the connectivity weights between artificial neurons and between regions to infer driving inputs to a region from all other recorded regions during different task conditions. Our preliminary results from two mice (six sessions each) suggest distinct bottom-up inputs for familiar images and top-down inputs for novel images in excitatory cell populations, on different days during a successful trial. Our modeling also suggests the role of inhibition by VIP neurons during task performance.</li></ul><h3 id=parallel-c-thursday-430-510-pm-edt>Parallel C, Thursday 4:30-5:10 PM EDT</h3><ul><li><strong>Yiqing Lu</strong>: While numerous theories attempts to explain the generation of phase precession in place cells, the underlying mechanism remains elusive. Here, we investigate the dual-input model (Fernández-Ruiz et al., 2017) as a potential mechanism for CA1 place cell phase precession, which proposes that the interplay of the strength and phases of location/time-dependent inputs from CA3 and mEC areas accounts for the phase advance of spikes in CA1 place cells relative to the theta rhythm. We apply a firing rate model to implement this idea. Our model considers an ensemble of CA1 place cells with identical place fields. We model the mean-membrane potentials in three compartments – soma, proximal dendrites, distal dendrites, the firing rate at soma and the mean synaptic output over the cells. With early mEC input innervating the distal dendrite and late CA3 input innervating the proximal dendrite, the model’s somatic firing rate shows phase precession (advance), consistent with experimental observations. Recent experiments (Wang, Foster & Pfeiffer, 2020) reported phase recession (delay) in a minority of place cells. Combining mathematical analysis and numerical simulation we show that our dual-input rate model can account for both phase precession and procession as the theta phases and amplitudes of the inputs vary. Furthermore, we extend the dual-input model to explain forward and backward theta sequence generation, where the place coding of distinct ensembles of place cells is compressed within a single theta-cycle to represent spatial trajectories.</li><li><strong>Chitaranjan Mahapatra</strong>: The firing patterns exhibited by layer II stellate cells in the medial entorhinal cortex play a crucial role in memory, cognition, and perception. These patterns are heavily influenced by the intricate subcellular calcium dynamics within the axon initial segment (AIS). Recent experimental findings have suggested a potential connection between dopamine D2 receptors (D2R) and T-type Ca2+ channels, mediated by a series of subcellular mechanisms. This proposed coupling provides an additional biophysical explanation for the modulation of firing patterns. Both the cAMP-PKA pathway and calcium influx are implicated in D2R-induced alterations in firing patterns. This in silico study seeks to deepen our comprehension of the calcium influx mediated by T-type Ca2+ channels through the cAMP-PKA pathway within the AIS of layer II stellate cells. Furthermore, this simulation study highlights their role in modulating resting membrane potential (RMP) and action potential (AP) plasticity, particularly in pathological conditions.</li><li><strong>Raphael Lafond-Mercier</strong>: For the weakly electrical fish Apteronotus Leptorhynchus, time and spatial memory formation seems to rely on spike rate adaptation in cells of their preglomerular complex (PG). A response is driven by the change in electric field around the fish caused by an encounter with an object. It is followed by a recovery from adaptation, with ultra-long time constants ranging from 0.5 to beyond 30 seconds. Certain PG cells are memoryless: adaptation depends only on the time since the most recent encounter. They provide a good encoding of the time elapsed ONLY between the two most recent encounters. Other PG cells have adaptation with memory, which could in principle contain information about a whole sequence of time intervals between successive encounters. Because the timings in the sequence are related to the distance between the objects triggering encounters, these memory cells are crucial for spatial learning. A comparison between two situations of food searching reveals the advantage of having memory cells. When food is reached right after leaving home, memoryless cells are always better than memory cells at estimating the total distance travelled. However, those same cells are useless in the case where an adaptation resetting event (encounter with an intermediate landmark) between home and food takes place. In contrast, memory cells show good performance, which means they contain information about the complete sequence of two time intervals. This memory adaptation mechanism is therefore possibly responsible for temporal and spatial encoding of the surroundings of the fish, allowing direct path integration.</li><li><strong>Daniel Levenstein</strong>: The mammalian hippocampus is involved in online navigation and memory formation, and can generate offline simulations for the purposes of recall, planning, and the consolidation of long term memories. Recently, it’s been found that spatially tuned cells similar to those seen in the hippocampus emerge in recurrent neural networks trained to predict upcoming sensory inputs, suggesting that prediction is a good candidate for unifying theory of hippocampal functions. However, whether prediction can provide a link between the online and offline operations of the hippocampus is unknown. Here, we show that predictive learning of sensory inputs can account for both online and offline operations of the hippocampus. However, the presence of spatially-tuned cells, which robustly emerge from all forms of predictive learning, does not guarantee the ability to engage in offline simulations. Offline simulations only emerged when networks used recurrent connections and head-direction information to predict multiple step sequences into the future, which promoted the formation of a continuous attractor manifold in which population activity reflected the spatial topology of the environment. Specifically, these networks generated realistic trajectories from noise and were able to replay recently experienced locations. Finally, a multi-step predictive algorithm inspired by sequential spiking in the hippocampus led to faster and more efficient learning. Altogether, our results demonstrate how continuous attractors can emerge in neural networks engaged in sequential predictive learning, which provides a unifying theory for hippocampal functions and hippocampal-inspired approaches to artificial intelligence.</li></ul><h3 id=parallel-a-friday-930-1000-am-edt>Parallel A, Friday 9:30-10:00 AM EDT</h3><ul><li><strong>Ines Qian</strong>: Recent in silico research by Rich et al. (2021, Cerebral Cortex) identified key differences in h-channel activity in human and rodent pyramidal neurons. Specifically, h-channel differences influenced the presence of subthreshold resonance in human L5 cortical pyramidal cells, leading to the hypothesis that a combination of the time constant of activation (mτ) and steady-state activation function (m∞) of the h-channel may play an important role in this frequency preference. To further understand how they relate to neuronal dynamics, we horizontally shifted the voltage dependence of the time constant of activation (mτ) and steady-state activation function (m∞) of the h-channel as presented by Rich et al. to determine whether such alterations would affect the neuron’s subthreshold frequency preference. Our preliminary results suggest that there is no apparent change in the neuron&rsquo;s frequency preference when shifts are introduced to the neuron&rsquo;s steady-state activation function (m∞). Conversely, a shift in the neurons&rsquo; time constant of activation (mτ) towards hyperpolarized voltages promotes subthreshold resonance at higher frequencies. This implies that the time constant of activation (mτ) holds greater significance in determining this neuron’s frequency preference. It can also serve as a further confirmation of the hypothesis that hyperpolarizing the human neuron will trigger speeding up in its h-current kinetics, resulting in a change in neuron’s subthreshold resonance, as discussed by Rich et al. The findings underline the importance of specifically studying the unique dynamics of human neurons.</li><li><strong>Adel Halawa</strong>: When presented with prolonged stimulation, sensory neurons must maintain effective information transfer despite bioenergetic constraints. We investigated spike-rate adaptation in rat hind paw low-threshold mechanoreceptors (LTMRs), recorded in vivo during 30-second vibrotactile stimulation. Our goal was to explore how adaptation, a change in encoding with prolonged stimulation, affects the mutual information between LTMR response and vibrotactile stimuli, as well as what the basis for that adoption is. Whereas spike rate quickly decreased during prolonged stimulation, spike-timing precision rapidly improved (i.e. jitter decreased), and stayed high throughout the trial. To examine how this happens, we fit neurons with general linear models (GLMs) during both early and late phases of the response. Adaptation was accounted for by changes in the refractoriness (h filter) and passive current (u) with minimal concomitant changes in the stimulus (k) filter. Stability of the stimulus filter suggests that neurons maintain the same frequency preference despite adaptation. Increased refractoriness might be regularizing the spike train. When looking at mutual information for a single cell response, we found it decreased with the adaptation response. However, when looking at population level activity, we found that information was lowest in the first second, after which it rapidly increased and stayed high. This followed a timeline similar to the change in linear filters, and improved spike-timing precision. All in all, our work shows that adaptation is beneficial for coding vibrotactile input by an ensemble of LTMRs, improving the temporal coding of stimulus frequency and the rate coding of stimulus amplitude.</li><li><strong>Sophie Schmidt-Hamkens</strong>: Even though neurite diameters play an important role for the electrophysiological behaviour of a neuron, precise measurements are rare. We introduce here data from complete dendrites of cortical neurons in the turtle for which local diameters were measured every $20 nm$ throughout the dendrite using super-resolution stimulated emission depletion microscopy (STED). The dendrites exhibit large low frequency diameter fluctuations with a cut-off at around $100$ per $mm$. Similarly noisy diameter profiles are observed in other high-quality reconstructions of dendrites using conventional microscopy albeit with much lower amplitudes. These results indicate a systematic bias in even the best among currently available morphological data. Interestingly, however, in our electrophysiology simulations, both passive synaptic integration and basic spiking behaviour seem just unaffected by the observed biological local diameter fluctuations. Riding on top of a tapering that optimises electrophysiological properties, diameter noise could serve space packing considerations in a regime where a neuron&rsquo;s function is not compromised.</li></ul><h3 id=parallel-b-friday-930-1000-am-edt>Parallel B, Friday 9:30-10:00 AM EDT</h3><ul><li><strong>Ali Hayadaroglu</strong>: Neurons in the mouse visual cortex are simultaneously driven by spontaneous movements and sensory signals. We asked how cortical distance, organization of sensory signals, and depth affect the distribution of spontaneous activity. We use Light Beads Microscopy (Demas et al, 2021) to functionally image Ca2+ transients at cellular resolution from large volumes (4x4x0.5mm^3) in GCamP6s mice. Awake mice were headfixed on a running wheel and recorded during spontaneous activity, natural image and retinotopic stimuli. We developed a novel volumetric cell extraction pipeline by extending to 3D and GPU-accelerating Suite2P (Pachitariu et al, 2017). First, we found small-scale structure in spontaneous activity: cells that highly correlate with running typically occur within ~100um patches. We found a weaker, large-scale organization: spontaneous correlations between cells decayed with distance up to 1.5 mm. Second, we found that spatial organization of sensory activity is not shared by spontaneous activity. Retinotopically-matched cells (distant cells which respond to the same visual location) had high sensory correlations while their spontaneous correlations were at chance level. Similarly, borders between V1 and higher visual areas had strong effects on sensory correlations, not on spontaneous correlations. Finally, we compared spontaneous activity across depths. Preliminary data showed that sensory responses are distributed evenly, while spontaneous activity drove L4 more strongly than L2/3. Our findings show that behaviourally-driven signals in visual cortex are spatially organized distinctly from sensory signals. This complements our understanding that these signals are orthogonal in neural space and can aid in understanding how they are integrated.</li><li><strong>Tajwar Sultana</strong>: The neurotic personality has an impact on the regulation of basic negative emotions such as anger, fear, and sadness. There has been extensive research in search of functional connectivity biomarkers of neuroticism and basic negative emotions but there is a lack of research based on effective connectivity. In the current research, we intended to determine the significance of causal interaction of three large-scale resting-state networks – default mode, salience, and executive networks – to predict neuroticism and basic negative emotions. In this study, a large-scale human connectome project dataset comprising functional MRI scans and self-reported scores of neuroticism and negative emotions of 1079 subjects, was utilized. Spectral dynamic causal modelling and parametric empirical Bayes was used to estimate the subject-level effective connectivity parameters and their group-level associations with the neuroticism and emotional scores. Leave-one-out cross-validation using parametric empirical Bayes was employed for prediction analysis. Our results for heightened emotions showed that the self-connection of right hippocampus can predict individuals with high fear, self-connections of dorsal anterior cingulate cortex, posterior cingulate cortex and left dorsolateral prefrontal cortex can predict individuals with high sadness. High anger, low sadness and neuroticism scores other than subject with low fear, could not be predicted using triple network effective connectivity. Our findings revealed that the causal (directed) connections of the resting-state triple network can potentially serve as a connectomic signature for people with high and low fear, high sadness, low anger, and neuroticism with low fear.</li></ul><h3 id=parallel-c-friday-930-1000-am-edt>Parallel C, Friday 9:30-10:00 AM EDT</h3><ul><li><strong>Anja Rabus</strong>: Does the brain optimize itself for storage and transmission of information and if so, how? The critical brain hypothesis is based in statistical physics and posits that the brain self-tunes its dynamics to a critical point or regime to maximize the repertoire of neuronal responses. Yet, the robustness of this regime, especially with respect to changes in the functional connectivity, remains an unsolved fundamental challenge. Here, we show that both scale-free neuronal dynamics and self-similar features of behavioral dynamics persist following significant changes in functional connectivity. Specifically, we find that the psychedelic compound ibogaine that is associated with an altered state of consciousness fundamentally alters the functional connectivity in the retrosplenial cortex of mice. Yet, the scale-free statistics of movement and of neuronal avalanches among behaviorally-related neurons remain largely unaltered. This indicates that the propagation of information within biological neural networks is robust to changes in functional organization of sub-populations of neurons, opening up a new perspective on how the adaptive nature of functional networks may lead to optimality of information transmission in the brain.</li><li><strong>Shervin Safavi</strong>: Perceptual multistability (when two or more percepts alternate in response to a single ambiguous sensory input) has been studied for centuries using myriad approaches, and has illuminated diverse cognitive functions (e.g., perceptual inference, attention, visual awareness) [1-3]. Traditionally, multistability has been viewed in Helmholtzian terms, i.e., treating perception as a passive [Bayesian] inference about the contents of the world [2,4]. However, this view neglects the crucial role played by value [3,5-8]: e.g., percepts paired with reward tend to dominate for longer periods than unpaired ones [5-6]. We reformulate visual multistability in terms of a decision process, employing the formalism of a partially observable Markov decision process (POMDP) [9]. Each percept is potentially associated with different sources of rewards or punishments (including aesthetic value [10]), and switching between percepts is a form of (costly) internal action - the attentional equivalent of the external action of moving eye gaze between objects. Selecting one percept is accompanied by reduced observation noise, and ultimately stronger beliefs about the perceived state (dominant percept). The solution of the POMDP is the (approximately) optimal perceptual policy; this replicates and explains several classic and elusive aspects of rivalry. Overall, our value-based decision-making account of perceptual multistability synergizes with previous accounts and also offers a more comprehensive treatment of computational and algorithmic facets of multistability.</li><li><strong>Martin Mittag</strong>: Numerous studies reveal the substantial degeneracy of ion channel distributions, where diverse parameter combinations result in comparable functional behaviors. The extent to which alternative parameter sets can be constrained by additional factors like energy consumption remains unclear. We will introduce a promising but not yet sufficiently explored idea to use Pareto optimality for simplifying the complex parameter and performance space spanned by populations of neurons. The assumption is that evolution selects neurons/circuits whose performance at one task cannot be improved without reducing their performance at other tasks. Such neurons are Pareto optimal. Neuronal systems that are fully optimal in a given situation, using a specific balance of tasks, would then be drawn from the set of Pareto optimal cells. However, the effective application of Pareto optimality principle to populations (or ensembles) of neuronal models requires a robust and consistent way to assess their performance on different tasks. We will show for neurons performing pattern separation (dentate granule cells) that the assessment of their functionality will be more reliable with information theoretic measures. In addition, we will showcase the application of the Pareto optimality to the trade-off between pattern separation and energy efficiency in hippocampal granule cell models. In summary, we will explore whether Pareto optimality can help address degeneracy by pinpointing the subpopulations of conductance-based models that strike the best balance between economy and functionality. Through this approach, the high-dimensional parameter space of neuronal models could potentially be reduced to geometrically simple low-dimensional manifolds.</li></ul></article><article id=05-past-future><h2 class=major>05 - Past & Future</h2><span class="image main"><img src alt></span><h2 id=previous-years>Previous Years</h2><h3 id=ccns-2022>CCNS 2022</h3><p>The full program of talks at CCNSv3, including sessions chaired by Dr. Alexandra Chatzikalymniou, Dr. Randy McIntosh, Dr. Milad Lankarany, and Drs. Andreea Diaconescu and Povilas Karvelis, can be found <a href=https://www.crowdcast.io/e/ccnsv3/register>here</a>.</p><h3 id=ccns-2021>CCNS 2021</h3><p>The full program of talks at CCNSv2, including sessions chaired by Dr. Etay Hay, Dr. Maurizio de Pitta, Dr. Carmen Canavier, and Dr. Bratislav Misic, can be found <a href=https://www.crowdcast.io/e/ccnsv2/register>here</a>.</p><h3 id=ccns-2020>CCNS 2020</h3><p>The full program of talks at CCNSv1, including keynote addresses from Dr. Nancy Kopell, Dr. Dimitris Pinotsis,
Dr. Cameron McIntyre, and Dr. Philip Corlett, can be found <a href=https://www.crowdcast.io/e/CCNS/register>here</a>.</p><h2 id=future-plans>Future Plans</h2><p>CCNS will return! Check this page, or follow the organizing committee on Twitter (<a href=https://twitter.com/RichCompNeuro>@RichCompNeuro</a>, <a href=https://twitter.com/cognemo_andreea>@cognemo_andreea</a>, <a href=https://twitter.com/neurodidact>@neurodidact</a>, and <a href=https://twitter.com/MLankarany>@MLankarany</a>) for the latest updates.</p><figure class="image main"><img src=images/new_brain_3.png></figure></article></div><footer id=footer><p class=copyright><img src=https://logos-download.com/wp-content/uploads/2016/02/Twitter_Logo_new.png width=20> #CCNS</p></footer></div><div id=bg></div></body><script src=https://krembilneuroinformatics.github.io/ccns/assets/js/jquery.min.js></script>
<script src=https://krembilneuroinformatics.github.io/ccns/assets/js/browser.min.js></script>
<script src=https://krembilneuroinformatics.github.io/ccns/assets/js/breakpoints.min.js></script>
<script src=https://krembilneuroinformatics.github.io/ccns/assets/js/util.js></script>
<script src=https://krembilneuroinformatics.github.io/ccns/assets/js/main.js></script></html>